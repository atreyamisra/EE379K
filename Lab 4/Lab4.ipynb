{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "import scipy.linalg as LA\n",
    "import matplotlib\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "from scipy.misc import imread\n",
    "from scipy.stats import skew\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.linear_model import Ridge, LassoCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 1: Linear Discriminant Analysis\n",
    "    Part 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean1 = [0, 0, 0]\n",
    "cov1 = [[1, 0.9, 0.9], [0.9, 1, 0.9], [0.9, 0.9, 1]]\n",
    "label1 = np.random.multivariate_normal(mean1, cov1, 20)\n",
    "\n",
    "mean2 = [0, 0, 1]\n",
    "cov2 = [[1, 0.8, 0.8], [0.8, 1, 0.8], [0.8, 0.8, 1]]\n",
    "label2 = np.random.multivariate_normal(mean2, cov2, 20)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(label1.T[0], label1.T[1], label1.T[2], c='r', label='Labels 1')\n",
    "ax.scatter(label2.T[0], label2.T[1], label2.T[2], c='b', label= 'Labels 2')\n",
    "\n",
    "ax.set_xlabel('X Axis')\n",
    "ax.set_ylabel('Y Axis')\n",
    "ax.set_zlabel('Z Axis')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just experimenting projection onto Z axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.scatter(label1.T[0], label1.T[1], c='r', label='Labels 1')\n",
    "ax.scatter(label2.T[0], label2.T[1], c='b', label= 'Labels 2')\n",
    "\n",
    "ax.set_xlabel('X Axis')\n",
    "ax.set_ylabel('Y Axis')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#calculate variance within class\n",
    "Sw = np.dot((label1-mean1).T, (label1-mean1)) + np.dot((label2-mean2).T, (label2-mean2))\n",
    "\n",
    "print Sw\n",
    "\n",
    "#calculate weights which maximize linear separation\n",
    "w = np.dot(np.linalg.inv(Sw), (np.subtract(mean2,mean1)))\n",
    "\n",
    "print \"vector of max weights\", w\n",
    "#projection of classes on 1D space\n",
    "plt.plot(np.dot(label1, w), [0]*len(label1), \"bo\", label=\"label1\")\n",
    "plt.plot(np.dot(label2, w), [0]*len(label2), \"go\", label=\"label2\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clf = LinearDiscriminantAnalysis()\n",
    "# clf.fit(label1, label2)\n",
    "\n",
    "\n",
    "# print(clf.predict([[-0.8, -1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Problem 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "all_data = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],\n",
    "                      test.loc[:,'MSSubClass':'SaleCondition']))\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "prices = pd.DataFrame({\"price\":train[\"SalePrice\"], \"log(price + 1)\":np.log1p(train[\"SalePrice\"])})\n",
    "#prices.hist()\n",
    "#log transform the target:\n",
    "train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n",
    "\n",
    "#log transform skewed numeric features:\n",
    "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
    "                               \n",
    "skewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\n",
    "skewed_feats = skewed_feats[skewed_feats > 0.75]\n",
    "skewed_feats = skewed_feats.index\n",
    "\n",
    "all_data[skewed_feats] = np.log1p(all_data[skewed_feats])\n",
    "\n",
    "all_data = pd.get_dummies(all_data)\n",
    "\n",
    "#filling NA's with the mean of the column:\n",
    "all_data = all_data.fillna(all_data.mean())\n",
    "\n",
    "#creating matrices for sklearn:\n",
    "X_train = all_data[:train.shape[0]]\n",
    "X_test = all_data[train.shape[0]:]\n",
    "y = train.SalePrice\n",
    "\n",
    "#creating a runner for alpha values\n",
    "def ridgeRegTest(a):\n",
    "    clf = Ridge(alpha=a)\n",
    "    clf.fit(X_train, y)\n",
    "    predict = np.expm1(clf.predict(X_test))\n",
    "\n",
    "    results = pd.DataFrame({\"Id\":test.Id, \"SalePrice\":predict})\n",
    "    results.to_csv(\"results.csv\", index = False)\n",
    "\n",
    "    results.plot(x='Id', y='SalePrice', kind='line')\n",
    "    plt.show()\n",
    "ridgeRegTest(.1)\n",
    "\n",
    "#checkeing rmse on different values of alpha\n",
    "alphas = []\n",
    "for i in range(0,200):\n",
    "    alphas.append(i*.01+9)\n",
    "    \n",
    "def rmse_cv(model):\n",
    "    rmse= np.sqrt(-cross_val_score(model, X_train, y, scoring=\"neg_mean_squared_error\", cv = 5))\n",
    "    return(rmse)\n",
    "#model_ridge = Ridge()\n",
    "\n",
    "#alphas = [.001, .01, .05, .1, .5, 1, 5, 10, 50]\n",
    "#first try small range then determine min from graph in order to save running time\n",
    "\n",
    "cv_ridge = [rmse_cv(Ridge(alpha = alpha)).mean() for alpha in alphas]\n",
    "\n",
    "cv_ridge = pd.Series(cv_ridge, index = alphas)\n",
    "# cv_ridge.plot(title = \"Validation - Just Do It\")\n",
    "# plt.xlabel(\"alpha\")\n",
    "# plt.ylabel(\"rmse\")\n",
    "# plt.show()\n",
    "\n",
    "print cv_ridge.min()\n",
    "\n",
    "alphas2 = []\n",
    "for i in range(0,10000):\n",
    "    alphas2.append(i*.0001)\n",
    "\n",
    "#model_lasso = LassoCV(alphas = alphas).fit(X_train, y)\n",
    "model_lasso = LassoCV()\n",
    "\n",
    "#cv_lasso = [rmse_cv(LassoCV(alphas = alpha)).mean()\n",
    "#            for alpha in alphas2]\n",
    "\n",
    "#cv_lasso = pd.Series(cv_lasso, index = alphas2)\n",
    "#cv_lasso.plot(title = \"Validation - Just Do It2\")\n",
    "#plt.xlabel(\"alpha\")\n",
    "#plt.ylabel(\"rmse\")\n",
    "#plt.show()\n",
    "#print alphas2\n",
    "print 'lasso mean:'\n",
    "print rmse_cv(model_lasso).mean()\n",
    "print 'lasso min:'\n",
    "print rmse_cv(model_lasso).min()\n",
    "\n",
    "\n",
    "model_lasso = LassoCV(alphas = [1, 0.5, 0.1, 0.001, 0.0005]).fit(X_train, y)\n",
    "print rmse_cv(model_lasso)\n",
    "print 'real lasso mean:'\n",
    "print rmse_cv(model_lasso).mean()\n",
    "print 'real lasso min:'\n",
    "print rmse_cv(model_lasso).min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Incorporating XGBoost (Part 5 onwards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label = y)\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "params = {\"max_depth\":2, \"eta\":0.1}\n",
    "model = xgb.cv(params, dtrain,  num_boost_round=500, early_stopping_rounds=100)\n",
    "\n",
    "model.loc[30:,[\"test-rmse-mean\", \"train-rmse-mean\"]].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_xgb = xgb.XGBRegression(n_estimators=360, max_depth=2, learning_rate=0.1)\n",
    "model_xgb.fit(X_train, y)\n",
    "\n",
    "xgb_preds = np.expm1(model_xgb.predict(X_test))\n",
    "\n",
    "results = pd.DataFrame({\"Id\":test.Id, \"SalePrice\":xgb_preds})\n",
    "results.to_csv(\"XGBresults.csv\", index = False)\n",
    "\n",
    "results.plot(x='Id', y='SalePrice', kind='line')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial XGBoost Submission resulted with .13427 (which is worse than the previous Ridge with CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "all_data = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],\n",
    "                      test.loc[:,'MSSubClass':'SaleCondition']))\n",
    "#log transform the target:\n",
    "train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n",
    "\n",
    "#log transform skewed numeric features:\n",
    "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
    "\n",
    "skewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\n",
    "skewed_feats = skewed_feats[skewed_feats > 0.75]\n",
    "skewed_feats = skewed_feats.index\n",
    "\n",
    "all_data[skewed_feats] = np.log1p(all_data[skewed_feats])\n",
    "\n",
    "all_data = pd.get_dummies(all_data)\n",
    "all_data = all_data.fillna(all_data.mean())\n",
    "X_train = all_data[:train.shape[0]]\n",
    "X_test = all_data[train.shape[0]:]\n",
    "y = train.SalePrice\n",
    "\n",
    "solver = sklearn.linear_model.RidgeCV(normalize=True)\n",
    "solver.fit(X_train, y)\n",
    "print(\"RidgeCV Score: \", solver.score(X_train, y))\n",
    "error = sqrt(sklearn.metrics.mean_squared_error(y, solver.predict(X_train)))\n",
    "print(\"RMSE of Training Predictions (RRCV): \", error)\n",
    "print()\n",
    "\n",
    "alphas = np.logspace(-100, 0, num = 100)\n",
    "\n",
    "solver = sklearn.linear_model.LassoCV(tol=0.1, normalize=True, alphas=alphas)\n",
    "solver.fit(X_train, y)\n",
    "print(\"LassoCV Score: \", solver.score(X_train, y))\n",
    "error = sqrt(sklearn.metrics.mean_squared_error(y, solver.predict(X_train)))\n",
    "print(\"RMSE of Training Predictions (LCV): \", error)\n",
    "print()\n",
    "\n",
    "alpha_path, coef_path, _ = sklearn.linear_model.lasso_path(X_train, y, alphas=alphas)\n",
    "\n",
    "l0_coeffs = [np.linalg.norm(coef, ord=0) for coef in coef_path.T]\n",
    "scores = [sklearn.metrics.r2_score(y, X_train.dot(coef)) for coef in coef_path.T]\n",
    "errors = [sklearn.metrics.mean_squared_error(y, X_train.dot(coef)) for coef in coef_path.T]\n",
    "idx = np.argmax(scores)\n",
    "\n",
    "print(\"Max Lasso Path Score: \", scores[idx])\n",
    "print(\"RMSE of Training Predictions (LPCV): \", errors[idx])\n",
    "print()\n",
    "\n",
    "#df = pd.DataFrame({\"id\":test.Id, \"SalePrice\":np.expm1(X_test.dot(coef_path.T[idx]))})\n",
    "#df.to_csv('Results.csv', index = False)\n",
    "\n",
    "plt.scatter(alpha_path, scores)\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Accuracy (R^2 Score)')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.scatter(alpha_path, l0_coeffs)\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('L0 Norm of Coefficients')\n",
    "plt.show()\n",
    "\n",
    "X_train['Results'] = X_train.dot(coef_path.T[-1])\n",
    "\n",
    "solver = sklearn.linear_model.RidgeCV(normalize=True)\n",
    "solver.fit(X_train, y)\n",
    "print(\"Ridge Score w/ outputs added to data: \", solver.score(X_train, y))\n",
    "error = sqrt(sklearn.metrics.mean_squared_error(y, solver.predict(X_train)))\n",
    "print(\"RMSE of Training Predictions (RR+CV): \", error)\n",
    "print()\n",
    "\n",
    "X_train = X_train.drop('Results', 1)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train.values, y.values)\n",
    "dtest = xgb.DMatrix(X_test.values)\n",
    "# specify parameters via map\n",
    "param = {'max_depth':2, 'eta':0.1, 'silent':1, 'subsample':0.5}\n",
    "num_round = 1000\n",
    "bst = xgb.train(param, dtrain, num_round)\n",
    "# make prediction\n",
    "preds = bst.predict(dtrain)\n",
    "error = sqrt(sklearn.metrics.mean_squared_error(y, preds))\n",
    "\n",
    "submission = bst.predict(dtest)\n",
    "#df = pd.DataFrame({\"id\":test.Id, \"SalePrice\":np.expm1(submission)})\n",
    "#df.to_csv('Results.csv', index = False)\n",
    "\n",
    "print(\"XGB Score: \", sklearn.metrics.r2_score(y, preds))\n",
    "print(\"RMSE of Training Predictions (XGB): \", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
